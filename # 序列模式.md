# 序列模式
分为以下4类：
1. 功能结构域 Domain
    ![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063156.jpg)
2. 模体 motif 也叫consenesus sequence
   ![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063158.jpg)
3. 模块 BLOCK
   ![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063200.jpg)
4. 模式 pattern/profile
    ![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063202.jpg)

## 模型的评估参数
其实就是混淆矩阵的构建求ACC，AUC，MCC等参数的过程。


## PSSM矩阵
PSSM矩阵是什么？
对获得的多条序列中的保守的BLOCK中的氨基酸频率进行统计的一个矩阵
它可以用于。。。。
### 如何构建
首先获得一系列BLOCK序列然后进行如下的三个步骤之一
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063203.jpg)
也就是：
1. 直接对某个位置的某个氨基酸的个数进行log运算
2. 直接对某个位置的某个氨基酸的个数
3. 直接计算某个位置的某个氨基酸的频率

### 应用
主要是用于传统的预测模型构建（与深度学习或者说机器学习的方法有所不同）
1. 可以用于naive贝叶斯模型的构建，因为矩阵中的每一个值代表的是某个氨基酸在某个位置出现的概率。
2. 计算log-odds ratio / Odds ratio。 这个也是可以用于预测模型的构建

#### log-odds ratio 
计算这个，首先需要有阳性训练集和阴性训练集。
分别对两个训练集计算PSSM矩阵或者说$p(S|+)$
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063205.jpg)
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063208.jpg)
得到两个这样的矩阵后如何进行数据的预测呢？
很简单
给定一个序列$S = S_1S_2S_3S_4...S_9$
并且已知阳性训练集的PSSM矩阵（图中的左部分）和背景数据（也就是阴性数据，这里取均等概率，如图右边）
这样便可以根据图中的方式计算Odds Ratio。
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063209.jpg)
而log Odds ratio 就是上述计算值取Log。
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063211.jpg)

>这样的方法最大的问题就是序列长度必须是固定的长度，如12bp等。但是实际上需要预测的序列往往会大于12bp或者其他，这个时候一个解决的方法就是输入的序列从左到右按12bp的滑窗进行滑动，这样可以得到许多子序列，这些子序列作为模型的输入去预测。![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063212.jpg)

最后与贝叶斯的方法进行对比 
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063215.jpg)
>虽然我认为两种方式都是基于测试数据和训练数据有着同样的分布，为什么我会这么认为呢？因为构建的PSSM矩阵是从训练集构建，odds ratio 仅仅是计算了一个阳性数据中出现这样个序列的概率与阴性数据出现这样一条序列的概率的比值似乎和贝叶斯的模型的想法是相同的？ 也可能是我个人的错误理解。
## 信息论
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063217.jpg)
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063218.jpg)
### 如何计算香农信息熵 
简单来说就是某个事件有多种可能性$P_1...P_n$
则这个时间的香农信息熵为：$H = \sum_{i=1}^{n}-P_i(log_2(P_i)$，这个值越大就说明事件结果的不确定性越大，而为0则说明没有不确定性。
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-063220.jpg)
而如果假设这样的BLOCK是一个motif，**可以根据如图的方式进行计算Motif在该位置的信息量**
![](https://tf-picture-bed-1259792641.cos.ap-beijing.myqcloud.com/blog/2021-08-18-63221.jpg)

## motif search agrithoms
### Gibbs Sampling Agrithoms
